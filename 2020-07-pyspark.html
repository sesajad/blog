<!DOCTYPE html><html><head><title>Coding Blog Boilerplate | PySpark Applications Dependencies</title><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"><meta name="robots" content="index,follow"><meta name="theme-color" content="#212121"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link rel="shortcut icon" href="/coding-blog-boilerplate/favicon.ico"><link href="https://fonts.googleapis.com/css?family=Hind:400,700&amp;display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:300,400&amp;display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/icon?family=Material+Icons|Material+Icons+Outlined" rel="stylesheet"><style>
      body, input, button {
        font-family: 'Hind', sans-serif;
      }

      code, .hljs {
        font-family: 'Source Code Pro', 'Courier New', Courier, monospace;
      }

      .icon-font {
        font-family: 'Material Icons';
        font-weight: normal;
        font-style: normal;
        font-size: 24px;  /* Preferred icon size */
        display: inline-block;
        line-height: 1;
        text-transform: none;
        letter-spacing: normal;
        word-wrap: normal;
        white-space: nowrap;
        direction: ltr;
      
        /* Support for all WebKit browsers. */
        -webkit-font-smoothing: antialiased;
        /* Support for Safari and Chrome. */
        text-rendering: optimizeLegibility;
      
        /* Support for Firefox. */
        -moz-osx-font-smoothing: grayscale;
      
        /* Support for IE. */
        font-feature-settings: 'liga';
      }

      .icon-font.outline {
        font-family: 'Material Icons Outlined';
      }
    </style><link href="/coding-blog-boilerplate/styles/codedoc-styles.css" rel="stylesheet"><script async="" defer="" src="/coding-blog-boilerplate/bundle/codedoc-bundle.js"></script><script>window.githubConfig={"repo":"coding-blog-boilerplate","user":"CONNECT-platform"}</script><style>.container{padding-top: 0 !important}</style><style>#-codedoc-toc { backdrop-filter: blur(16px); -webkit-backdrop-filter: blur(16px) }</style><meta name="twitter:title" content="Coding Blog Boilerplate | PySpark Applications Dependencies"><meta property="og:type" content="article"><meta property="og:title" content="Coding Blog Boilerplate | PySpark Applications Dependencies"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"></head><body><div class="header-0-0-12"><a class="watermark-0-0-11" href="https://coding.blog" target="_blank"><svg viewBox="0 0 701 443" version="1.1" xmlns="http://www.w3.org/2000/svg"><g id="coding.blog" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g transform="translate(-162.000000, -246.000000)" id="Shape"><path d="M538.081948,246.348452 L538.52402,246.445719 L538.52402,246.445719 L538.85529,246.526578 L538.85529,246.526578 L600.956012,263.165455 C602.767076,263.660918 604.526214,264.465865 606.146589,265.591976 C611.80881,269.527048 614.090389,276.211909 612.345003,282.123825 L538.2641,559.728905 C537.816127,561.562539 536.989561,563.335054 535.765607,564.944016 C530.70443,571.597248 520.764856,572.934442 513.564947,567.930724 C507.902727,563.995652 505.621148,557.310791 507.366534,551.398875 L577.145082,289.91439 L546.233523,281.631663 L462.305576,595.917544 L468.439004,634.645619 L493.907962,603.194573 C499.468987,596.327279 509.54413,595.268339 516.411424,600.829364 C523.278717,606.390389 524.337658,616.465532 518.776633,623.332826 L470.948283,682.395919 C466.887248,687.410879 460.418878,689.3283 454.55045,687.832105 L454.212574,687.741996 C448.224449,686.200133 443.421088,681.240355 442.392221,674.744345 L430.503202,599.680031 C430.418831,599.147339 430.361793,598.616628 430.331048,598.089327 C429.783912,595.705405 429.838457,593.220092 430.535776,590.858154 L519.361965,258.223717 L519.447729,257.889555 L519.447729,257.889555 L519.532157,257.586199 L519.627058,257.23399 C520.074771,255.398716 520.901697,253.624577 522.126677,252.014264 C525.828762,247.147643 532.141006,245.125338 538.081948,246.348452 Z M632.156421,656.157784 C640.992977,656.157784 648.156421,663.321228 648.156421,672.157784 C648.156421,680.99434 640.992977,688.157784 632.156421,688.157784 L528.156421,688.157784 C519.319865,688.157784 512.156421,680.99434 512.156421,672.157784 C512.156421,663.321228 519.319865,656.157784 528.156421,656.157784 L632.156421,656.157784 Z M699.234631,342.452157 L857.62655,500.844076 C863.874939,507.092464 863.874939,517.223104 857.62655,523.471493 L699.234631,681.863412 C692.986243,688.1118 682.855603,688.1118 676.607214,681.863412 C670.358826,675.615023 670.358826,665.484383 676.607214,659.235995 L823.686132,512.157077 L676.607214,365.079574 C670.358826,358.831185 670.358826,348.700545 676.607214,342.452157 C682.855603,336.203768 692.986243,336.203768 699.234631,342.452157 Z M347.705627,342.452157 C353.954016,348.700545 353.954016,358.831185 347.705627,365.079574 L200.62671,512.158491 L347.705627,659.235995 C353.954016,665.484383 353.954016,675.615023 347.705627,681.863412 C341.457239,688.1118 331.326599,688.1118 325.07821,681.863412 L166.686292,523.471493 C160.437903,517.223104 160.437903,507.092464 166.686292,500.844076 L325.07821,342.452157 C331.326599,336.203768 341.457239,336.203768 347.705627,342.452157 Z"></path></g></g></svg></a></div><div id="-codedoc-container" class="container"><script>window.source = {"base":"posts","path":"2020-07-pyspark.md","namespace":"/coding-blog-boilerplate","title":{"base":"Coding Blog Boilerplate","connector":" | "}}</script><div class="hero-0-0-1" data-target="desktop" style="margin-bottom: -156px"><img src="/coding-blog-boilerplate/img/2020-07-pyspark-header.svg" class="image-0-0-2" data-hero=""><span class="caption-0-0-3"></span></div><h1 class="h-0-0-4 white"><span style=" 
      text-shadow: 0 0 8px black; 
      font-size: 48px"><p>PySpark Applications Dependencies</p></span></h1><script id="bxRFVEokog">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("bxRFVEokog", "ylYa/4ufj9QlJdFvnn+Y6w==", {"src":"github"});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); })()</script><p>originally published in Sep 2018</p><p>(if you’re just looking for the answer, it’s at the end of the article)</p><p>let’s ignore all of the folks about the advantages and disadvantages of PySpark in comparison to spark. here is an article for whom is constrained to use PySpark.</p><h2 id="problem" class="heading-0-0-5"><span class="anchor-0-0-6" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Problem</h2><p>while exploring data you may encounter such a problem that you need to do a complex mapping process, for example, converting base64 string of images into arrays which represents an image. or even more complicated, such as embedding data using a pre-trained model to detect anomalies. first of all, we must make sure that the tasks are Map-Reduce and Big Data tasks, otherwise, we may need to handle them out of Spark.</p><p>Then, we realize that we need a third-party library to be used on the mapping task (or reduce or anything), this is the start of great pain.</p><h2 id="search-for-solutions" class="heading-0-0-5"><span class="anchor-0-0-6" data-ignore-text=""><span class="icon-font" data-ignore-text="" style="vertical-align: sub">link</span></span>Search for Solutions</h2><p>the simple dirty solution is to install the needed library on the python environments on all nodes of the cluster but please :))</p><p>after a bunch of searches, we found out that there is a method called <code>SparkContext.addPyFile</code> which as documentation said it supports .py, .zip and .egg on local, HTTP and HDFS. the very first point is that egg is dead and is succeeded by wheel. <strong>if the library is a single file</strong> (like gists), this function may be found useful, also if the desired library still has a fresh egg file (which I recommend to not use that library) it can be simply used by this function. (even it’s not guaranteed to work for the reason I will say)</p><p>PySpark just adds the archive to <code>sys.path</code> so, you cannot simply add .whl files to it, you must do something like pip does during installation. the best way is to call pip to extract the package.</p><p>so something like this may works</p><pre class=""><code class="python code-0-0-7" tabindex="0"><span class="wmbar-0-0-10"><span></span><span></span><span></span><span></span></span><div class="line-0-0-9 " data-content="import subprocess"><span class="lineCounter-0-0-8 prim">1</span><span class="token keyword">import</span> subprocess</div><br><div class="line-0-0-9 " data-content="import shutil"><span class="lineCounter-0-0-8">2</span><span class="token keyword">import</span> shutil</div><br><div class="line-0-0-9 " data-content="def install(package):"><span class="lineCounter-0-0-8">3</span><span class="token keyword">def</span> <span class="token function">install</span><span class="token punctuation">(</span>package<span class="token punctuation">)</span><span class="token punctuation">:</span></div><br><div class="line-0-0-9 " data-content="    subprocess.call([sys.executable, '-m', 'pip', '-t', '/tmp/pysparkpackages/%s/' % package]"><span class="lineCounter-0-0-8">4</span>    subprocess<span class="token punctuation">.</span>call<span class="token punctuation">(</span><span class="token punctuation">[</span>sys<span class="token punctuation">.</span>executable<span class="token punctuation">,</span> <span class="token string">'-m'</span><span class="token punctuation">,</span> <span class="token string">'pip'</span><span class="token punctuation">,</span> <span class="token string">'-t'</span><span class="token punctuation">,</span> <span class="token string">'/tmp/pysparkpackages/%s/'</span> <span class="token operator">%</span> package<span class="token punctuation">]</span></div><br><div class="line-0-0-9 " data-content="    shutil.make_archive('/tmp/pysparkpackages/arch_%s' % package, 'zip', '/tmp/pysparkpackages/%s/' % package)"><span class="lineCounter-0-0-8 prim">5</span>    shutil<span class="token punctuation">.</span>make_archive<span class="token punctuation">(</span><span class="token string">'/tmp/pysparkpackages/arch_%s'</span> <span class="token operator">%</span> package<span class="token punctuation">,</span> <span class="token string">'zip'</span><span class="token punctuation">,</span> <span class="token string">'/tmp/pysparkpackages/%s/'</span> <span class="token operator">%</span> package<span class="token punctuation">)</span></div><br><div class="line-0-0-9 " data-content="    sc.addPyFile('/tmp/pysparkpackages/arch_%s' % package)"><span class="lineCounter-0-0-8 prim">6</span>    sc<span class="token punctuation">.</span>addPyFile<span class="token punctuation">(</span><span class="token string">'/tmp/pysparkpackages/arch_%s'</span> <span class="token operator">%</span> package<span class="token punctuation">)</span></div><br></code></pre><p>note that <code>pip -t dir package</code> installs the package in dir and doesn’t add it to installed packages list (<code>pip freeze</code>)</p><p>it works, but for simple libraries. for libraries such as TensorFlow or numpy which contains .so files, this doesn’t work.</p><p>(because PySpark add_files for zip files depends on zipimport and due to <a href="https://www.python.org/dev/peps/pep-0273/">PEP0273</a> “import of dynamic modules (*.pyd, *.so) is disallowed.”.</p><p>then, we realize that <code>sc.addPyFile</code> is not much useful for adding libraries, now we’re looking for a way to install library temporarily on all of the nodes.</p><p>first, to make a function run on all of the nodes, there is no simple way in spark, therefore we need to use this hack</p><pre class=""><code class="python code-0-0-7" tabindex="0"><span class="wmbar-0-0-10"><span></span><span></span><span></span><span></span></span><div class="line-0-0-9 " data-content="def run_on_all_workers(x)"><span class="lineCounter-0-0-8 prim">1</span><span class="token keyword">def</span> <span class="token function">run_on_all_workers</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span></div><br><div class="line-0-0-9 " data-content="    ... process ..."><span class="lineCounter-0-0-8">2</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> process <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></div><br><div class="line-0-0-9 " data-content="    return x"><span class="lineCounter-0-0-8">3</span>    <span class="token keyword">return</span> x</div><br><div class="line-0-0-9 " data-content="a large_rdd = a_large_rdd.mapPartitions(run_on_all_workers)"><span class="lineCounter-0-0-8 prim">4</span>a large_rdd <span class="token operator">=</span> a_large_rdd<span class="token punctuation">.</span>mapPartitions<span class="token punctuation">(</span>run_on_all_workers<span class="token punctuation">)</span></div><br></code></pre><p>and now, the only way to correctly install a .whl library is to install it using pip, and one of the ways to make it temporary is to install in <code>SparkFiles.getRootDirectory()</code> which is a per-application temporary directory.</p><p>so, the final word is:</p><p><strong>if you want to install a third-party library such as TensorFlow on a Spark cluster, you can run following code on Zeppelin</strong></p><pre class=""><code class="python code-0-0-7" tabindex="0"><span class="wmbar-0-0-10"><span></span><span></span><span></span><span></span></span><div class="line-0-0-9 " data-content="%pyspark"><span class="lineCounter-0-0-8 prim">1</span><span class="token operator">%</span>pyspark</div><br><div class="line-0-0-9 " data-content="def install_deps(x):"><span class="lineCounter-0-0-8">2</span><span class="token keyword">def</span> <span class="token function">install_deps</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span></div><br><div class="line-0-0-9 " data-content="    from pyspark import SparkFiles"><span class="lineCounter-0-0-8">3</span>    <span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkFiles</div><br><div class="line-0-0-9 " data-content="    from subprocess import call"><span class="lineCounter-0-0-8">4</span>    <span class="token keyword">from</span> subprocess <span class="token keyword">import</span> call</div><br><div class="line-0-0-9 " data-content="    import sys"><span class="lineCounter-0-0-8 prim">5</span>    <span class="token keyword">import</span> sys</div><br><div class="line-0-0-9 " data-content="    for package in ['tensorflow', 'keras']: # or any other library"><span class="lineCounter-0-0-8">6</span>    <span class="token keyword">for</span> package <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'tensorflow'</span><span class="token punctuation">,</span> <span class="token string">'keras'</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token comment"># or any other library</span></div><br><div class="line-0-0-9 " data-content="        call([sys.executable, '-m', 'pip', 'install', '-t', SparkFiles.getRootDirectory(), package])"><span class="lineCounter-0-0-8">7</span>        call<span class="token punctuation">(</span><span class="token punctuation">[</span>sys<span class="token punctuation">.</span>executable<span class="token punctuation">,</span> <span class="token string">'-m'</span><span class="token punctuation">,</span> <span class="token string">'pip'</span><span class="token punctuation">,</span> <span class="token string">'install'</span><span class="token punctuation">,</span> <span class="token string">'-t'</span><span class="token punctuation">,</span> SparkFiles<span class="token punctuation">.</span>getRootDirectory<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> package<span class="token punctuation">]</span><span class="token punctuation">)</span></div><br><div class="line-0-0-9 " data-content="    return x"><span class="lineCounter-0-0-8">8</span>    <span class="token keyword">return</span> x</div><br><div class="line-0-0-9 " data-content="sc.parallelize(range(0,100)).mapPartitions(install_deps).collect()"><span class="lineCounter-0-0-8 prim">9</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mapPartitions<span class="token punctuation">(</span>install_deps<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span></div><br></code></pre><p><strong>you can change</strong> <code>sc.parallelize(range(0,100))</code> <strong>with your data RDD.</strong></p><div class="contentnav-0-0-17" data-no-search=""><a href="#problem" class="h2" data-content-highlight="problem">Problem</a><a href="#search-for-solutions" class="h2" data-content-highlight="search-for-solutions">Search for Solutions</a></div></div><div id="-codedoc-toc" class="toc-0-0-14"><div class="content-0-0-15"><p><a href="/coding-blog-boilerplate/">Home</a>
<a href="/coding-blog-boilerplate/2020-07-xyz">2020-07-xyz</a>
<a href="/coding-blog-boilerplate/2020-07-pyspark">2020-07-pyspark</a></p></div></div><div class="footer-0-0-13"><div class="left"><script id="iSFCbOJVaF">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("iSFCbOJVaF", "56wK23hnpB5qisWh57QS+Q==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); })()</script></div><div class="main"><div class="inside"></div></div><div class="right"><script id="hiIvWafEFt">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("hiIvWafEFt", "3BUwzqQzsN9L9NjFnOzTSQ==", {});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); })()</script></div></div><script id="iJJt__cShK">(function(){function load(){if (window.__sdh_transport){window.__sdh_transport("iJJt__cShK", "hHqXMJQoCuKATcWej33b0w==", {"namespace":"/coding-blog-boilerplate"});} }; if (document.readyState == 'complete') load(); else window.addEventListener('load', load); })()</script></body></html>